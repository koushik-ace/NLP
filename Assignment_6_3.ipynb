{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushik-ace/NLP/blob/main/Assignment_6_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA with sample data\n"
      ],
      "metadata": {
        "id": "4rRHP_23L770"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/LDA-Data.xlsx\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "p5U4R_m7IyMo",
        "outputId": "fd6bb355-e660-43b0-f9f5-3fba226cf0bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             News\n",
              "0   Virat scored century in match\n",
              "1            BJP won in elections\n",
              "2  Bumra took 5 wicket in a match\n",
              "3  Congress form state government"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-948c1dfb-7eb0-415b-921e-7bbbd7cb7277\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Virat scored century in match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BJP won in elections</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bumra took 5 wicket in a match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Congress form state government</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-948c1dfb-7eb0-415b-921e-7bbbd7cb7277')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-948c1dfb-7eb0-415b-921e-7bbbd7cb7277 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-948c1dfb-7eb0-415b-921e-7bbbd7cb7277');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"News\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"BJP won in elections\",\n          \"Congress form state government\",\n          \"Virat scored century in match\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data (including punkt_tab)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Changed from 'punkt' to 'punkt_tab'\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Handle non-string inputs\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # 1. Clean Text: convert to lowercase, remove non-alphabetic characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "    # 2. Word Tokenization\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # 3. Stopword removal & 4. Lemmatization\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words and len(word) > 1:  # Remove single character words\n",
        "            processed_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # 5. Rejoin\n",
        "    return ' '.join(processed_words)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df['News'] = df['News'].fillna('')\n",
        "\n",
        "# Apply the preprocessing function to the 'News' column\n",
        "df['Processed_News'] = df['News'].apply(preprocess_text)\n",
        "\n",
        "print(\"Original News and Processed News:\")\n",
        "print(df[['News', 'Processed_News']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbn6u3tQKIJN",
        "outputId": "493149b9-e951-45fe-cea4-c451708da4d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original News and Processed News:\n",
            "                             News                  Processed_News\n",
            "0   Virat scored century in match      virat scored century match\n",
            "1            BJP won in elections                    bjp election\n",
            "2  Bumra took 5 wicket in a match         bumra took wicket match\n",
            "3  Congress form state government  congress form state government\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the 'Processed_News' column to create the BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(df['Processed_News'])\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Vocabulary (Feature Names):\")\n",
        "print(feature_names)\n",
        "print(\"\\nShape of BoW matrix:\", bow_matrix.shape)\n",
        "\n",
        "# To display a part of the BoW matrix, convert it to a DataFrame (optional, for better viewing)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
        "print(\"\\nFirst 5 rows of the BoW matrix:\")\n",
        "print(bow_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtU_4oXIKPDi",
        "outputId": "21b871b6-15c9-45a6-e04c-13ed48051c02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Feature Names):\n",
            "['bjp' 'bumra' 'century' 'congress' 'election' 'form' 'government' 'match'\n",
            " 'scored' 'state' 'took' 'virat' 'wicket']\n",
            "\n",
            "Shape of BoW matrix: (4, 13)\n",
            "\n",
            "First 5 rows of the BoW matrix:\n",
            "   bjp  bumra  century  congress  election  form  government  match  scored  \\\n",
            "0    0      0        1         0         0     0           0      1       1   \n",
            "1    1      0        0         0         1     0           0      0       0   \n",
            "2    0      1        0         0         0     0           0      1       0   \n",
            "3    0      0        0         1         0     1           1      0       0   \n",
            "\n",
            "   state  took  virat  wicket  \n",
            "0      0     0      1       0  \n",
            "1      0     0      0       0  \n",
            "2      0     1      0       1  \n",
            "3      1     0      0       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuAcPo4aKeBy",
        "outputId": "4d5df628-08bb-49fd-c4ce-829e3dfc1048"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bjp  bumra  century  congress  election  form  government  match  scored  \\\n",
            "0    0      0        1         0         0     0           0      1       1   \n",
            "1    1      0        0         0         1     0           0      0       0   \n",
            "2    0      1        0         0         0     0           0      1       0   \n",
            "3    0      0        0         1         0     1           1      0       0   \n",
            "\n",
            "   state  took  virat  wicket  \n",
            "0      0     0      1       0  \n",
            "1      0     0      0       0  \n",
            "2      0     1      0       1  \n",
            "3      1     0      0       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Define the number of topics (you can change this based on your needs)\n",
        "num_topics = 2\n",
        "\n",
        "# Initialize LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "\n",
        "# Fit the model to the BoW matrix\n",
        "lda_output = lda_model.fit_transform(bow_matrix)\n",
        "\n",
        "# Display the topics and their top words\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_): # Corrected from components__ to components_\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 5\n",
        "print(\"\\nLDA Topics:\")\n",
        "display_topics(lda_model, feature_names, no_top_words)\n",
        "\n",
        "# Add the dominant topic to the original DataFrame\n",
        "df['Dominant_Topic'] = lda_output.argmax(axis=1)\n",
        "\n",
        "print(\"\\nDataFrame with Dominant Topic:\")\n",
        "print(df[['News', 'Dominant_Topic']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1Ag-VFTKtdl",
        "outputId": "8c752d5c-dcfb-47c4-d80d-1d298e3b1c24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA Topics:\n",
            "Topic 0:\n",
            "form government congress state election\n",
            "Topic 1:\n",
            "match virat century scored took\n",
            "\n",
            "DataFrame with Dominant Topic:\n",
            "                             News  Dominant_Topic\n",
            "0   Virat scored century in match               1\n",
            "1            BJP won in elections               0\n",
            "2  Bumra took 5 wicket in a match               1\n",
            "3  Congress form state government               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the topics and their top words again in this cell\n",
        "# Reusing the 'display_topics' function, 'lda_model', 'feature_names', and 'no_top_words' from previous execution\n",
        "print(\"LDA Topics:\")\n",
        "display_topics(lda_model, feature_names, no_top_words)\n",
        "\n",
        "print(\"\\nDataFrame with News and their Dominant Topic:\")\n",
        "print(df[['News', 'Dominant_Topic']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFs-gE1GLTze",
        "outputId": "8dc5d2bb-9ad8-4316-976e-836dbe92070e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA Topics:\n",
            "Topic 0:\n",
            "form government congress state election\n",
            "Topic 1:\n",
            "match virat century scored took\n",
            "\n",
            "DataFrame with News and their Dominant Topic:\n",
            "                             News  Dominant_Topic\n",
            "0   Virat scored century in match               1\n",
            "1            BJP won in elections               0\n",
            "2  Bumra took 5 wicket in a match               1\n",
            "3  Congress form state government               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA with kaggle data"
      ],
      "metadata": {
        "id": "J1f1JG4qMCXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Try with on_bad_lines parameter\n",
        "df = pd.read_csv(\"/content/arxiv_data.csv\",\n",
        "                 on_bad_lines='skip',  # Skip problematic rows\n",
        "                 engine='python')       # Use Python engine (more forgiving)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fdDv2zwhMHQT",
        "outputId": "c4d2f610-9a05-461a-f305-15e40f60839c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-135ec5cd-4262-415e-b808-e178308d7b7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-135ec5cd-4262-415e-b808-e178308d7b7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-135ec5cd-4262-415e-b808-e178308d7b7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-135ec5cd-4262-415e-b808-e178308d7b7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51774,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38972,\n        \"samples\": [\n          \"Sum-Product-Transform Networks: Exploiting Symmetries using Invertible Transformations\",\n          \"A Primal-Dual Subgradient Approachfor Fair Meta Learning\",\n          \"Adversarial Multi-Source Transfer Learning in Healthcare: Application to Glucose Prediction for Diabetic People\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38979,\n        \"samples\": [\n          \"Continual learning (CL) is a setting in which an agent has to learn from an\\nincoming stream of data during its entire lifetime. Although major advances\\nhave been made in the field, one recurring problem which remains unsolved is\\nthat of Catastrophic Forgetting (CF). While the issue has been extensively\\nstudied empirically, little attention has been paid from a theoretical angle.\\nIn this paper, we show that the impact of CF increases as two tasks\\nincreasingly align. We introduce a measure of task similarity called the NTK\\noverlap matrix which is at the core of CF. We analyze common projected gradient\\nalgorithms and demonstrate how they mitigate forgetting. Then, we propose a\\nvariant of Orthogonal Gradient Descent (OGD) which leverages structure of the\\ndata through Principal Component Analysis (PCA). Experiments support our\\ntheoretical findings and show how our method can help reduce CF on classical CL\\ndatasets.\",\n          \"Few-shot learning is a challenging task since only few instances are given\\nfor recognizing an unseen class. One way to alleviate this problem is to\\nacquire a strong inductive bias via meta-learning on similar tasks. In this\\npaper, we show that such inductive bias can be learned from a flat collection\\nof unlabeled images, and instantiated as transferable representations among\\nseen and unseen classes. Specifically, we propose a novel part-based\\nself-supervised representation learning scheme to learn transferable\\nrepresentations by maximizing the similarity of an image to its discriminative\\npart. To mitigate the overfitting in few-shot classification caused by data\\nscarcity, we further propose a part augmentation strategy by retrieving extra\\nimages from a base dataset. We conduct systematic studies on miniImageNet and\\ntieredImageNet benchmarks. Remarkably, our method yields impressive results,\\noutperforming the previous best unsupervised methods by 7.74% and 9.24% under\\n5-way 1-shot and 5-way 5-shot settings, which are comparable with\\nstate-of-the-art supervised methods.\",\n          \"Surgical instrument segmentation is extremely important for computer-assisted\\nsurgery. Different from common object segmentation, it is more challenging due\\nto the large illumination and scale variation caused by the special surgical\\nscenes. In this paper, we propose a novel bilinear attention network with\\nadaptive receptive field to solve these two challenges. For the illumination\\nvariation, the bilinear attention module can capture second-order statistics to\\nencode global contexts and semantic dependencies between local pixels. With\\nthem, semantic features in challenging areas can be inferred from their\\nneighbors and the distinction of various semantics can be boosted. For the\\nscale variation, our adaptive receptive field module aggregates multi-scale\\nfeatures and automatically fuses them with different weights. Specifically, it\\nencodes the semantic relationship between channels to emphasize feature maps\\nwith appropriate scales, changing the receptive field of subsequent\\nconvolutions. The proposed network achieves the best performance 97.47% mean\\nIOU on Cata7 and comes first place on EndoVis 2017 by 10.10% IOU overtaking\\nsecond-ranking method.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3157,\n        \"samples\": [\n          \"['cs.LG', 'cs.CE', 'q-fin.ST', 'stat.ML']\",\n          \"['cs.LG', 'physics.comp-ph', 'physics.flu-dyn']\",\n          \"['cs.LG', 'cs.CV', 'math.AT']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the CSV file with error handling\n",
        "df = pd.read_csv(\"/content/arxiv_data.csv\",\n",
        "                 on_bad_lines='skip',\n",
        "                 engine='python')\n",
        "\n",
        "print(f\"Loaded {len(df)} rows\")\n",
        "print(\"Columns in dataset:\", df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Handle non-string inputs\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # 1. Clean Text: convert to lowercase, remove non-alphabetic characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "    # 2. Word Tokenization\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # 3. Stopword removal & 4. Lemmatization\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words and len(word) > 1:  # Remove single character words\n",
        "            processed_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # 5. Rejoin\n",
        "    return ' '.join(processed_words)\n",
        "\n",
        "# Handle missing values in 'titles' and 'summaries' by filling with empty strings\n",
        "df['titles'] = df['titles'].fillna('')\n",
        "df['summaries'] = df['summaries'].fillna('')\n",
        "\n",
        "# Combine 'titles' and 'summaries' into a new 'text_content' column\n",
        "df['text_content'] = df['titles'] + ' ' + df['summaries']\n",
        "\n",
        "# Apply the preprocessing function to the 'text_content' column\n",
        "print(\"\\nPreprocessing text... This may take a while for large datasets.\")\n",
        "df['Processed_Text'] = df['text_content'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Original Text Content and Processed Text:\")\n",
        "print(\"=\"*70)\n",
        "print(df[['text_content', 'Processed_Text']].head())\n",
        "\n",
        "# Additional useful information\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Dataset Statistics:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Rows with empty processed text: {(df['Processed_Text'] == '').sum()}\")\n",
        "print(f\"Average processed text length: {df['Processed_Text'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DcNAYaGORov",
        "outputId": "d02a0735-51af-4ac4-e3a7-1ff158ec69c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 51774 rows\n",
            "Columns in dataset: ['titles', 'summaries', 'terms']\n",
            "\n",
            "First few rows:\n",
            "                                              titles  \\\n",
            "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
            "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
            "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
            "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
            "4  Background-Foreground Segmentation for Interio...   \n",
            "\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                         terms  \n",
            "0           ['cs.CV', 'cs.LG']  \n",
            "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
            "2           ['cs.CV', 'cs.AI']  \n",
            "3                    ['cs.CV']  \n",
            "4           ['cs.CV', 'cs.LG']  \n",
            "\n",
            "Preprocessing text... This may take a while for large datasets.\n",
            "\n",
            "======================================================================\n",
            "Original Text Content and Processed Text:\n",
            "======================================================================\n",
            "                                        text_content  \\\n",
            "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
            "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
            "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
            "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
            "4  Background-Foreground Segmentation for Interio...   \n",
            "\n",
            "                                      Processed_Text  \n",
            "0  survey semantic stereo matching semantic depth...  \n",
            "1  future ai guiding principle consensus recommen...  \n",
            "2  enforcing mutual consistency hard region semi ...  \n",
            "3  parameter decoupling strategy semi supervised ...  \n",
            "4  background foreground segmentation interior se...  \n",
            "\n",
            "======================================================================\n",
            "Dataset Statistics:\n",
            "======================================================================\n",
            "Total rows: 51774\n",
            "Rows with empty processed text: 0\n",
            "Average processed text length: 982.16 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the 'Processed_Text' column to create the BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(df['Processed_Text'])\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Bag of Words (BoW) Analysis\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nVocabulary Size:\", len(feature_names))\n",
        "print(\"\\nFirst 20 words in vocabulary:\")\n",
        "print(feature_names[:20])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BoW Matrix Shape:\", bow_matrix.shape)\n",
        "print(f\"  - Number of documents: {bow_matrix.shape[0]}\")\n",
        "print(f\"  - Number of unique words: {bow_matrix.shape[1]}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# To display a part of the BoW matrix, convert it to a DataFrame (optional, for better viewing)\n",
        "# Note: Only convert a sample if the matrix is very large\n",
        "print(\"\\nConverting BoW matrix to DataFrame (this may take a moment for large datasets)...\")\n",
        "\n",
        "# For large datasets, only show first few rows and columns\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "print(\"\\nFirst 5 rows of the BoW matrix (showing first 10 columns):\")\n",
        "print(bow_df.iloc[:5, :10])\n",
        "\n",
        "print(\"\\nFull BoW matrix for first 5 documents (all features):\")\n",
        "print(bow_df.head())\n",
        "\n",
        "# Show some statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BoW Statistics:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total word occurrences: {bow_matrix.sum()}\")\n",
        "print(f\"Average words per document: {bow_matrix.sum(axis=1).mean():.2f}\")\n",
        "print(f\"Sparsity: {(1 - bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1])) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUgnzTozPdjH",
        "outputId": "f6bf485f-66b8-4fdd-e331-1b01cb969ee8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Bag of Words (BoW) Analysis\n",
            "======================================================================\n",
            "\n",
            "Vocabulary Size: 51361\n",
            "\n",
            "First 20 words in vocabulary:\n",
            "['aa' 'aaa' 'aaae' 'aaai' 'aabb' 'aabo' 'aac' 'aaca' 'aachen' 'aacp'\n",
            " 'aacvp' 'aad' 'aada' 'aadcnn' 'aade' 'aadi' 'aadit' 'aads' 'aadsah' 'aae']\n",
            "\n",
            "======================================================================\n",
            "BoW Matrix Shape: (51774, 51361)\n",
            "  - Number of documents: 51774\n",
            "  - Number of unique words: 51361\n",
            "======================================================================\n",
            "\n",
            "Converting BoW matrix to DataFrame (this may take a moment for large datasets)...\n",
            "\n",
            "First 5 rows of the BoW matrix (showing first 10 columns):\n",
            "   aa  aaa  aaae  aaai  aabb  aabo  aac  aaca  aachen  aacp\n",
            "0   0    0     0     0     0     0    0     0       0     0\n",
            "1   0    0     0     0     0     0    0     0       0     0\n",
            "2   0    0     0     0     0     0    0     0       0     0\n",
            "3   0    0     0     0     0     0    0     0       0     0\n",
            "4   0    0     0     0     0     0    0     0       0     0\n",
            "\n",
            "Full BoW matrix for first 5 documents (all features):\n",
            "   aa  aaa  aaae  aaai  aabb  aabo  aac  aaca  aachen  aacp  ...  zxhresearch  \\\n",
            "0   0    0     0     0     0     0    0     0       0     0  ...            0   \n",
            "1   0    0     0     0     0     0    0     0       0     0  ...            0   \n",
            "2   0    0     0     0     0     0    0     0       0     0  ...            0   \n",
            "3   0    0     0     0     0     0    0     0       0     0  ...            0   \n",
            "4   0    0     0     0     0     0    0     0       0     0  ...            0   \n",
            "\n",
            "   zy  zyj  zykls  zynq  zynqnet  zypda  zzg  zzh  zzzjzzz  \n",
            "0   0    0      0     0        0      0    0    0        0  \n",
            "1   0    0      0     0        0      0    0    0        0  \n",
            "2   0    0      0     0        0      0    0    0        0  \n",
            "3   0    0      0     0        0      0    0    0        0  \n",
            "4   0    0      0     0        0      0    0    0        0  \n",
            "\n",
            "[5 rows x 51361 columns]\n",
            "\n",
            "======================================================================\n",
            "BoW Statistics:\n",
            "======================================================================\n",
            "Total word occurrences: 6281256\n",
            "Average words per document: 121.32\n",
            "Sparsity: 99.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with restrictions\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=1000,      # Only use top 1000 most frequent words\n",
        "    min_df=5,               # Ignore words appearing in less than 5 documents\n",
        "    max_df=0.8              # Ignore words appearing in more than 80% of documents\n",
        ")\n",
        "\n",
        "# Fit and transform the 'Processed_Text' column to create the BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(df['Processed_Text'])\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Vocabulary (Feature Names):\")\n",
        "print(feature_names)\n",
        "print(\"\\nShape of BoW matrix:\", bow_matrix.shape)\n",
        "\n",
        "# To display a part of the BoW matrix, convert it to a DataFrame (optional, for better viewing)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
        "print(\"\\nFirst 5 rows of the BoW matrix:\")\n",
        "print(bow_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeY8xDlnQ6Rn",
        "outputId": "cc564b98-7042-40b7-f730-39b8bb8d83df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Feature Names):\n",
            "['ability' 'able' 'according' 'account' 'accuracy' 'accurate' 'accurately'\n",
            " 'achieve' 'achieved' 'achieves' 'achieving' 'across' 'action'\n",
            " 'activation' 'active' 'activity' 'actor' 'adapt' 'adaptation' 'adaptive'\n",
            " 'addition' 'additional' 'additionally' 'address' 'advance' 'advantage'\n",
            " 'adversarial' 'agent' 'aggregation' 'agnostic' 'ai' 'aim' 'al'\n",
            " 'algorithm' 'alignment' 'allow' 'allowing' 'allows' 'along' 'also'\n",
            " 'alternative' 'although' 'among' 'amount' 'analysis' 'analyze'\n",
            " 'annotated' 'annotation' 'anomaly' 'another' 'appearance' 'application'\n",
            " 'applied' 'apply' 'applying' 'approach' 'approximate' 'approximation'\n",
            " 'arbitrary' 'architecture' 'area' 'art' 'artificial' 'aspect'\n",
            " 'associated' 'assumption' 'attack' 'attempt' 'attention' 'attribute'\n",
            " 'augmentation' 'augmented' 'auto' 'autoencoder' 'automated' 'automatic'\n",
            " 'automatically' 'autonomous' 'auxiliary' 'available' 'average' 'aware'\n",
            " 'backbone' 'background' 'base' 'based' 'baseline' 'batch' 'bayesian'\n",
            " 'become' 'behavior' 'benchmark' 'benefit' 'besides' 'best' 'better'\n",
            " 'beyond' 'bias' 'binary' 'black' 'block' 'body' 'boost' 'bound'\n",
            " 'boundary' 'bounding' 'box' 'brain' 'branch' 'build' 'building' 'built'\n",
            " 'called' 'camera' 'capability' 'capable' 'caption' 'captioning' 'capture'\n",
            " 'captured' 'case' 'category' 'causal' 'cell' 'certain' 'challenge'\n",
            " 'challenging' 'change' 'channel' 'characteristic' 'choice' 'cifar'\n",
            " 'class' 'classical' 'classification' 'classifier' 'clinical' 'cloud'\n",
            " 'cluster' 'clustering' 'cnn' 'cnns' 'co' 'coarse' 'coco' 'code'\n",
            " 'collected' 'collection' 'color' 'com' 'combination' 'combine' 'combined'\n",
            " 'combining' 'common' 'commonly' 'communication' 'community' 'comparable'\n",
            " 'compare' 'compared' 'comparison' 'competitive' 'complete' 'completion'\n",
            " 'complex' 'complexity' 'component' 'comprehensive' 'compression'\n",
            " 'computation' 'computational' 'computationally' 'compute' 'computer'\n",
            " 'computing' 'concept' 'condition' 'conditional' 'conduct' 'conducted'\n",
            " 'connected' 'connection' 'consider' 'considered' 'considering'\n",
            " 'consistency' 'consistent' 'consists' 'constrained' 'constraint'\n",
            " 'construct' 'contains' 'content' 'context' 'contextual' 'continuous'\n",
            " 'contrast' 'contrastive' 'contribution' 'control' 'conventional'\n",
            " 'convergence' 'convex' 'convolution' 'convolutional' 'correlation'\n",
            " 'correspondence' 'corresponding' 'cost' 'could' 'critic' 'critical'\n",
            " 'cross' 'crucial' 'cue' 'current' 'data' 'database' 'dataset' 'datasets'\n",
            " 'deal' 'decision' 'decoder' 'decomposition' 'deep' 'defined'\n",
            " 'demonstrate' 'demonstrated' 'dense' 'density' 'dependency' 'dependent'\n",
            " 'depth' 'derive' 'derived' 'describe' 'description' 'descriptor' 'design'\n",
            " 'designed' 'despite' 'detail' 'detect' 'detecting' 'detection' 'detector'\n",
            " 'develop' 'developed' 'development' 'device' 'diagnosis' 'difference'\n",
            " 'different' 'differentiable' 'difficult' 'difficulty' 'dimension'\n",
            " 'dimensional' 'direction' 'directly' 'discovery' 'discrete'\n",
            " 'discriminative' 'discriminator' 'discus' 'disease' 'distance'\n",
            " 'distillation' 'distributed' 'distribution' 'diverse' 'diversity' 'dnn'\n",
            " 'dnns' 'domain' 'downstream' 'driven' 'driving' 'dual' 'due' 'dynamic'\n",
            " 'easily' 'edge' 'effect' 'effective' 'effectively' 'effectiveness'\n",
            " 'efficiency' 'efficient' 'efficiently' 'effort' 'either' 'element'\n",
            " 'embedded' 'embedding' 'embeddings' 'empirical' 'empirically' 'employ'\n",
            " 'enable' 'enables' 'encode' 'encoder' 'encoding' 'end' 'energy' 'enhance'\n",
            " 'ensemble' 'entity' 'entropy' 'environment' 'error' 'especially'\n",
            " 'essential' 'estimate' 'estimating' 'estimation' 'estimator' 'etc'\n",
            " 'evaluate' 'evaluated' 'evaluation' 'even' 'event' 'every' 'example'\n",
            " 'existing' 'expensive' 'experience' 'experiment' 'experimental' 'expert'\n",
            " 'explain' 'explainable' 'explanation' 'explicit' 'explicitly' 'exploit'\n",
            " 'exploiting' 'exploration' 'explore' 'expression' 'extend' 'extensive'\n",
            " 'extract' 'extracted' 'extraction' 'face' 'facial' 'fact' 'factor' 'fail'\n",
            " 'far' 'fashion' 'fast' 'faster' 'feature' 'field' 'filter' 'final'\n",
            " 'finally' 'find' 'finding' 'fine' 'first' 'fixed' 'flexible' 'flow'\n",
            " 'focus' 'following' 'forecasting' 'forest' 'form' 'formulation' 'forward'\n",
            " 'found' 'four' 'frame' 'framework' 'free' 'frequency' 'full' 'fully'\n",
            " 'function' 'fundamental' 'furthermore' 'fusion' 'future' 'gain' 'game'\n",
            " 'gan' 'gans' 'gap' 'gaussian' 'gcn' 'general' 'generalization'\n",
            " 'generalize' 'generalized' 'generally' 'generate' 'generated' 'generates'\n",
            " 'generating' 'generation' 'generative' 'generator' 'generic' 'geometric'\n",
            " 'geometry' 'github' 'give' 'given' 'global' 'gnn' 'gnns' 'goal' 'good'\n",
            " 'gradient' 'grained' 'graph' 'great' 'grid' 'ground' 'group' 'guarantee'\n",
            " 'guide' 'guided' 'hand' 'handle' 'hard' 'head' 'help' 'hence'\n",
            " 'heterogeneous' 'hidden' 'hierarchical' 'high' 'higher' 'highly'\n",
            " 'however' 'http' 'human' 'hybrid' 'idea' 'identification' 'identify'\n",
            " 'identity' 'ii' 'image' 'imagenet' 'imaging' 'impact' 'implementation'\n",
            " 'implicit' 'importance' 'important' 'improve' 'improved' 'improvement'\n",
            " 'improves' 'improving' 'including' 'incorporate' 'increase' 'increasing'\n",
            " 'independent' 'individual' 'inference' 'influence' 'information'\n",
            " 'initial' 'input' 'insight' 'inspired' 'instance' 'instead' 'inter'\n",
            " 'interaction' 'interest' 'interpretability' 'interpretable'\n",
            " 'interpretation' 'introduce' 'introduced' 'introduces' 'invariant'\n",
            " 'inverse' 'investigate' 'issue' 'iterative' 'joint' 'jointly' 'kernel'\n",
            " 'key' 'kitti' 'knowledge' 'known' 'label' 'labeled' 'labeling' 'lack'\n",
            " 'language' 'large' 'latent' 'layer' 'lead' 'leading' 'learn' 'learned'\n",
            " 'learning' 'learns' 'length' 'less' 'level' 'leverage' 'leveraging'\n",
            " 'lidar' 'light' 'like' 'likelihood' 'limit' 'limitation' 'limited' 'line'\n",
            " 'linear' 'link' 'literature' 'local' 'localization' 'location' 'long'\n",
            " 'loss' 'low' 'lower' 'lstm' 'machine' 'made' 'main' 'mainly' 'major'\n",
            " 'make' 'making' 'manifold' 'manner' 'many' 'map' 'mapping' 'margin'\n",
            " 'markov' 'mask' 'match' 'matching' 'matrix' 'maximum' 'may' 'mean'\n",
            " 'meaningful' 'measure' 'measurement' 'mechanism' 'medical' 'memory'\n",
            " 'meta' 'method' 'methodology' 'metric' 'missing' 'mixture' 'ml' 'mobile'\n",
            " 'modal' 'modality' 'mode' 'model' 'modeling' 'modern' 'module'\n",
            " 'molecular' 'molecule' 'monocular' 'moreover' 'motion' 'motivated'\n",
            " 'moving' 'much' 'multi' 'multimodal' 'multiple' 'multivariate' 'mutual'\n",
            " 'named' 'namely' 'natural' 'nature' 'navigation' 'need' 'negative'\n",
            " 'neighbor' 'net' 'network' 'neural' 'new' 'next' 'node' 'noise' 'noisy'\n",
            " 'non' 'nonlinear' 'normal' 'normalization' 'novel' 'number' 'numerical'\n",
            " 'object' 'objective' 'observation' 'observed' 'obtain' 'obtained'\n",
            " 'occlusion' 'offer' 'often' 'one' 'online' 'open' 'operation' 'operator'\n",
            " 'optical' 'optimal' 'optimization' 'optimize' 'optimized' 'order'\n",
            " 'original' 'outperform' 'outperforms' 'output' 'overall' 'overcome'\n",
            " 'pair' 'paper' 'paradigm' 'parameter' 'part' 'partial' 'particular'\n",
            " 'particularly' 'past' 'patch' 'path' 'patient' 'pattern' 'pedestrian'\n",
            " 'per' 'perception' 'perform' 'performance' 'performed' 'performing'\n",
            " 'performs' 'person' 'perspective' 'perturbation' 'phase' 'photo'\n",
            " 'physical' 'pipeline' 'pixel' 'planning' 'play' 'point' 'policy'\n",
            " 'pooling' 'popular' 'pose' 'position' 'positive' 'possible' 'potential'\n",
            " 'power' 'powerful' 'practical' 'practice' 'pre' 'precision' 'predict'\n",
            " 'predicted' 'predicting' 'prediction' 'predictive' 'present' 'presented'\n",
            " 'preserve' 'preserving' 'previous' 'previously' 'prior' 'privacy'\n",
            " 'probabilistic' 'probability' 'problem' 'procedure' 'process'\n",
            " 'processing' 'produce' 'product' 'progress' 'projection' 'promising'\n",
            " 'propagation' 'property' 'proposal' 'propose' 'proposed' 'proposes'\n",
            " 'prove' 'provide' 'provided' 'provides' 'providing' 'pruning' 'pseudo'\n",
            " 'public' 'publicly' 'purpose' 'pyramid' 'qualitative' 'quality'\n",
            " 'quantitative' 'query' 'question' 'random' 'range' 'rank' 'rate' 'rather'\n",
            " 'raw' 'real' 'realistic' 'reasoning' 'recent' 'recently' 'recognition'\n",
            " 'reconstruction' 'recurrent' 'reduce' 'reduces' 'reduction' 'reference'\n",
            " 'region' 'registration' 'regression' 'regularization' 'reinforcement'\n",
            " 'related' 'relation' 'relationship' 'relative' 'relevant' 'rely'\n",
            " 'remains' 'report' 'represent' 'representation' 'require' 'required'\n",
            " 'requirement' 'requires' 'research' 'researcher' 'residual' 'resnet'\n",
            " 'resolution' 'resource' 'respect' 'respectively' 'result' 'resulting'\n",
            " 'retrieval' 'review' 'reward' 'rgb' 'rich' 'risk' 'rl' 'road' 'robot'\n",
            " 'robust' 'robustness' 'role' 'rotation' 'rule' 'saliency' 'salient'\n",
            " 'sample' 'sampling' 'scalable' 'scale' 'scenario' 'scene' 'scheme'\n",
            " 'score' 'search' 'second' 'seen' 'segment' 'segmentation' 'selection'\n",
            " 'self' 'semantic' 'semi' 'sensing' 'sensitive' 'sensor' 'sentence'\n",
            " 'sequence' 'sequential' 'series' 'set' 'setting' 'several' 'shape'\n",
            " 'shared' 'shift' 'short' 'shot' 'show' 'showing' 'shown' 'signal'\n",
            " 'significant' 'significantly' 'similar' 'similarity' 'simple'\n",
            " 'simulation' 'simultaneously' 'since' 'single' 'size' 'small' 'social'\n",
            " 'solution' 'solve' 'solving' 'source' 'space' 'sparse' 'spatial' 'spatio'\n",
            " 'specific' 'specifically' 'spectral' 'speed' 'sr' 'stage' 'standard'\n",
            " 'state' 'statistical' 'step' 'stereo' 'still' 'stochastic' 'strategy'\n",
            " 'stream' 'strong' 'structural' 'structure' 'structured' 'student'\n",
            " 'studied' 'study' 'style' 'sub' 'subject' 'subset' 'success' 'successful'\n",
            " 'successfully' 'suffer' 'suitable' 'super' 'superior' 'superiority'\n",
            " 'supervised' 'supervision' 'support' 'surface' 'survey' 'synthesis'\n",
            " 'synthetic' 'system' 'tabular' 'tackle' 'take' 'target' 'task' 'teacher'\n",
            " 'technique' 'temporal' 'tensor' 'term' 'test' 'tested' 'testing' 'text'\n",
            " 'texture' 'theoretical' 'theory' 'therefore' 'three' 'thus' 'time'\n",
            " 'together' 'tool' 'top' 'topic' 'topology' 'towards' 'tracking'\n",
            " 'traditional' 'traffic' 'train' 'trained' 'training' 'trajectory'\n",
            " 'transfer' 'transform' 'transformation' 'transformer' 'transition'\n",
            " 'translation' 'tree' 'truth' 'tuning' 'two' 'type' 'typically'\n",
            " 'uncertainty' 'underlying' 'understand' 'understanding' 'unified' 'unit'\n",
            " 'unknown' 'unlabeled' 'unlike' 'unseen' 'unsupervised' 'update' 'upon'\n",
            " 'us' 'use' 'used' 'useful' 'user' 'using' 'usually' 'utilize' 'validate'\n",
            " 'value' 'variable' 'variance' 'variant' 'variation' 'variational'\n",
            " 'variety' 'various' 'varying' 'vector' 'vehicle' 'version' 'via' 'video'\n",
            " 'view' 'vision' 'visual' 'volume' 'way' 'weakly' 'weight' 'weighted'\n",
            " 'well' 'whether' 'whole' 'wide' 'widely' 'wise' 'within' 'without' 'word'\n",
            " 'work' 'world' 'would' 'year' 'yet' 'yield' 'zero']\n",
            "\n",
            "Shape of BoW matrix: (51774, 1000)\n",
            "\n",
            "First 5 rows of the BoW matrix:\n",
            "   ability  able  according  account  accuracy  accurate  accurately  achieve  \\\n",
            "0        0     0          0        0         1         0           0        0   \n",
            "1        0     0          0        0         0         0           0        0   \n",
            "2        0     0          0        0         0         0           0        0   \n",
            "3        1     0          0        0         0         0           0        0   \n",
            "4        0     0          0        0         0         0           0        0   \n",
            "\n",
            "   achieved  achieves  ...  within  without  word  work  world  would  year  \\\n",
            "0         0         0  ...       0        0     0     0      0      0     0   \n",
            "1         0         0  ...       0        0     0     0      0      0     0   \n",
            "2         0         0  ...       0        0     0     0      0      0     0   \n",
            "3         2         0  ...       0        0     0     0      0      1     0   \n",
            "4         0         0  ...       0        0     0     2      0      0     1   \n",
            "\n",
            "   yet  yield  zero  \n",
            "0    0      0     0  \n",
            "1    0      0     0  \n",
            "2    0      0     0  \n",
            "3    0      0     0  \n",
            "4    0      0     0  \n",
            "\n",
            "[5 rows x 1000 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN18SPV+TMQKAYKpA7mfMW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}